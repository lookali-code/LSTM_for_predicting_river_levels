#Requires these packages and python 3.8-3.9 (3.8 is what is used here) (some packages may not be necessary due to changes made during preliminary tests)
import os
import gc
import numpy as np
import pandas as pd
import esig
from esig import stream2logsig
import iisignature
from functools import reduce
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
esig.set_backend("iisignature")
import tensorflow as tf
from pathlib import Path
# Limit GPU usage
# This stops errors appearing and computer getting super loud. Useful when limited by hardware
#gpus = tf.config.experimental.list_physical_devices('GPU')
#if gpus:
    #try:
        #for gpu in gpus:
            #tf.config.experimental.set_memory_growth(gpu, True)
   # except RuntimeError as e:
       # print(e)


# Setting up files. These files contain the values rergistered over 10 years at each location
# Define base directory relative to the current script
base_dir = Path("data")
# Dictionary of relative file paths
files = {
    'value1': base_dir / "Aldwark-Bridge-level-15min-Qualified.csv",
    'value2': base_dir / "Knaresborough-level-15min-Qualified.csv",
    'value3': base_dir / "Myton-On-Swale-level-15min-Qualified.csv",
    'value4': base_dir / "Ripon-Ure-Bank-level-15min-Qualified.csv",
    'value5': base_dir / "Westwick-Lock-level-15min-Qualified.csv",
    'value6': base_dir / "Masham-level-15min-Qualified.csv",
    'value7': base_dir / "Skip-Br.-K.Hammerton-flow-15min-Qualified.csv",
    'value8': base_dir / "Skip-Br.-K.Hammerton-level-15min-Qualified.csv",
    'value9': base_dir / "Crakehill-Topcliffe-flow-15min-Qualified.csv",
    'value10': base_dir / "Crakehill-Topcliffe-level-15min-Qualified.csv",
    'value11': base_dir / "Grinton-Bridge-level-15min-Qualified.csv",
    'value12': base_dir / "Kilgram-Bridge-flow-15min-Qualified.csv",
    'value13': base_dir / "Ure-level-level-15min-Qualified.csv",
    'predicting': base_dir / "Viking-Recorder-level-15min-Qualified.csv"
}


# Pre-process data
data_frames = []
for feature, file_path in files.items():
    df = pd.read_csv(file_path, low_memory = False)

    if 'dateTime' in df.columns and 'value' in df.columns:
        df = df[['dateTime', 'value']]
    else:
        print(f"Warning: Missing 'dateTime' or 'value' column in {file_path}")
        continue

    df['dateTime'] = pd.to_datetime(df['dateTime'], errors='coerce')
    df = df.sort_values(by='dateTime')
    df = df.rename(columns={'value': feature})
    df[feature] = df[feature].interpolate(method='linear')
    data_frames.append(df)

merged_data = reduce(lambda left, right: pd.merge(left, right, on="dateTime", how="inner"), data_frames)
#CHANGE WINDOW SIZE 
# Set up window for taking values 
start_date = pd.to_datetime('2014-01-01')
end_date = pd.to_datetime('2024-06-30')
window_size = pd.DateOffset(hours=12)
step_size = pd.DateOffset(hours=3)

merged_data['dateTime'] = pd.to_datetime(merged_data['dateTime'])
merged_data.set_index('dateTime', inplace=True)

# Lists to store extracted features
signatures = []
last_values = []
start_values_list = []
prediction_timestamps = []
# Sliding window splitting up values
current_start_date = start_date
#CHANGE THE HOURS=X HERE TO BE PREDICTION HORIZON
while current_start_date + window_size + pd.DateOffset(hours=24) <= end_date:
    current_end_date = current_start_date + window_size
    data_window = merged_data.loc[current_start_date:current_end_date]

    times = data_window.index.values
    values = data_window.drop(columns=['predicting']).values

    times_numeric = np.array([pd.Timestamp(t).timestamp() for t in times])
    times_normalized = times_numeric - times_numeric[0]

    scaler = StandardScaler()
    scaled_values = scaler.fit_transform(values)

    sig =  stream2logsig(scaled_values, 4)
    signatures.append(sig)
	#CHANGE THE HOURS=X HERE TO BE PREDICTION HORIZON (USE minutes=x FOR SHORTER PREDICTION HORIZON
    last_value_predicting = merged_data.loc[current_end_date + pd.DateOffset(hours=24), 'predicting']
    last_values.append(last_value_predicting)

    prediction_timestamps.append(current_end_date + pd.DateOffset(hours=24))

    start_values = data_window.drop(columns=['predicting']).iloc[0].values
    start_values_list.append(start_values)

    current_start_date += step_size


# Convert to NumPy arrays
signature_matrix = np.array(signatures)
start_values_array = np.array(start_values_list)
end_values_array = np.array(last_values)

# Add the start values to the  log-signatures 
final_feature_matrix = np.hstack((signature_matrix, start_values_array))
del data_frames, signatures  # Free up memory to work faster
gc.collect()

# Reshape the data for the Neural Net
X = final_feature_matrix.reshape((final_feature_matrix.shape[0], 1, final_feature_matrix.shape[1]))
y = end_values_array

# Split data for training and testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Build LSTM model
model = Sequential()
model.add(LSTM(units=256, activation='tanh', input_shape=(X.shape[1], X.shape[2]), return_sequences=True))
model.add(LSTM(units=64, activation='tanh'))
model.add(Dense(1))
model.compile(optimizer=Adam(learning_rate=0.0001), loss='mean_squared_error')

# Train model
history = model.fit(X_train, y_train, epochs=100, batch_size=64, validation_data=(X_test, y_test), verbose=1)
# Make predictions on the test data set
predictions = model.predict(X_test)
# Evaluate model (a quick check)
mse = mean_squared_error(y_test, predictions)
print(f"Mean Squared Error: {mse}")

# Set up output directories
model_dir = Path("models")
output_dir = Path("outputs")

model_dir.mkdir(exist_ok=True)
output_dir.mkdir(exist_ok=True)

# Generate a unique filename with timestamp
from datetime import datetime
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
model_filename = model_dir / f"RNN_{timestamp}.h5"
predictions_filename = output_dir / f"predictions_{timestamp}.csv"

# Save the model
model.save(model_filename)
print(f"Model saved as '{model_filename}'")

# Save predictions to CSV
predictions_df.to_csv(predictions_filename, index=False)
print(f"Predictions saved to '{predictions_filename}'")

test_timestamps = prediction_timestamps[len(X_train):]

predictions_df = pd.DataFrame({
    'Times': test_timestamps,
    'Actual': y_test.flatten(),
    'Predicted': predictions.flatten()
})

#USED SIMPLY TO SEE IF THE NETWORK RAN CORRECTLY. CHANGING LABELS PROVIDES THE PLOTS SEEN IN THE PAPER (THIS WAS DONE AFTER USING THE SAVED PREDICTIONS AND A DIFFERENT CODE FILE FOR EASE)
# Plot actual vs predicted values of Viking recorder using the original times
plt.figure(figsize=(10, 6))
plt.plot(test_timestamps, y_test, label='Actual data readings', color='blue')
plt.plot(test_timestamps, predictions, label='Predicted values', color='red', linestyle='dashed')
plt.title('A plot of Actual River Level readings compared to Predicted River Levels')
plt.xlabel('Date and Time')
plt.ylabel('River Level Height (m)')
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
